{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File directory: C:/Users/HP/Desktop/SIH/fuzzy/chess.txt\n",
      "\n",
      "\n",
      "Chess is a board game played between two players. It is sometimes called Western chess or international chess to distinguish it from related games such as xiangqi and shogi. The current form of the game emerged in Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin. Today, chess is one of the world's most popular games, played by millions of people worldwide.\n",
      "\n",
      "Chess is an abstract strategy game and involves no hidden information. It is played on a square chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player (one controlling the white pieces, the other controlling the black pieces) controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in \"check\") and there is no way for it to escape. There are also several ways a game can end in a draw.\n",
      "\n",
      "Organized chess arose in the 19th century. Chess competition today is governed internationally by FIDE (International Chess Federation). The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion. A huge body of chess theory has developed since the game's inception. Aspects of art are found in chess composition; and chess in its turn influenced Western culture and art and has connections with other fields such as mathematics, computer science, and psychology.\n",
      "\n",
      "One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Today's chess engines are significantly stronger than the best human players, and have deeply influenced the development of chess theory.\n",
      "\n",
      "\n",
      "Sentences: ['Chess is a board game played between two players.', 'It is sometimes called Western chess or international chess to distinguish it from related games such as xiangqi and shogi.', 'The current form of the game emerged in Southern Europe during the second half of the 15th century after evolving from chaturanga, a similar but much older game of Indian origin.', \"Today, chess is one of the world's most popular games, played by millions of people worldwide.\", 'Chess is an abstract strategy game and involves no hidden information.', 'It is played on a square chessboard with 64 squares arranged in an eight-by-eight grid.', 'At the start, each player (one controlling the white pieces, the other controlling the black pieces) controls sixteen pieces: one king, one queen, two rooks, two bishops, two knights, and eight pawns.', 'The object of the game is to checkmate the opponent\\'s king, whereby the king is under immediate attack (in \"check\") and there is no way for it to escape.', 'There are also several ways a game can end in a draw.', 'Organized chess arose in the 19th century.', 'Chess competition today is governed internationally by FIDE (International Chess Federation).', 'The first universally recognized World Chess Champion, Wilhelm Steinitz, claimed his title in 1886; Magnus Carlsen is the current World Champion.', \"A huge body of chess theory has developed since the game's inception.\", 'Aspects of art are found in chess composition; and chess in its turn influenced Western culture and art and has connections with other fields such as mathematics, computer science, and psychology.', 'One of the goals of early computer scientists was to create a chess-playing machine.', 'In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov.', \"Today's chess engines are significantly stronger than the best human players, and have deeply influenced the development of chess theory.\"]\n",
      "\n",
      "\n",
      "bitokens feature vector: [5, 10, 17, 9, 6, 6, 23, 12, 5, 4, 8, 15, 7, 16, 7, 13, 12]\n",
      "\n",
      "\n",
      "tritokens feature vector: [4, 9, 16, 8, 5, 5, 22, 11, 4, 3, 7, 14, 6, 15, 6, 12, 11]\n",
      "\n",
      "\n",
      "sentence position: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "\n",
      "\n",
      "Total number of sentences: 17\n",
      "\n",
      "\n",
      "Sentence position feature vector: [1, 0.9411764705882353, 0.8823529411764706, 0.8235294117647058, 0.7647058823529411, 0.7058823529411765, 0.6470588235294118, 0.5882352941176471, 0.5294117647058824, 0.47058823529411764, 0.4117647058823529, 0.35294117647058826, 0.29411764705882354, 0.23529411764705882, 0.17647058823529413, 0.11764705882352941, 1]\n",
      "\n",
      "\n",
      "SentenceVectors: [[0, 0, 0, 1, 2, 0, 0, 4, 5, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 7, 1, 0, 1, 2, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 2, 0], [0, 0, 0, 4, 0, 1, 0, 14, 8, 0, 4, 4, 0, 0, 0, 0, 0, 0, 7, 4, 0, 0, 0, 0, 0, 0, 11, 5, 0, 11, 6, 9, 1, 5, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 1, 0, 19, 0, 0, 0, 1, 0, 2], [0, 0, 1, 7, 1, 1, 0, 2, 10, 1, 5, 7, 1, 0, 0, 0, 0, 1, 11, 5, 0, 0, 0, 0, 0, 0, 17, 9, 0, 8, 11, 10, 0, 14, 1, 0, 1, 0, 0, 2, 0, 0, 4, 0, 0, 0, 7, 1, 0, 1, 30, 0, 0, 0, 0, 1, 8], [0, 0, 0, 1, 5, 0, 0, 7, 4, 1, 5, 3, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 8, 2, 0, 4, 10, 2, 0, 3, 1, 0, 2, 0, 0, 0, 0, 0, 7, 0, 0, 0, 2, 0, 0, 0, 15, 0, 0, 0, 0, 3, 1], [0, 0, 0, 2, 0, 0, 0, 6, 7, 1, 3, 2, 0, 0, 0, 0, 0, 0, 7, 1, 0, 1, 0, 0, 0, 0, 5, 2, 0, 5, 4, 5, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 10, 0, 0, 0, 0, 1, 0], [1, 0, 0, 4, 1, 1, 0, 6, 8, 2, 4, 0, 0, 0, 0, 2, 0, 0, 4, 1, 0, 0, 0, 0, 0, 1, 7, 4, 0, 6, 2, 4, 2, 6, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 2, 2], [0, 0, 0, 5, 6, 0, 0, 11, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 14, 8, 1, 0, 1, 4, 0, 5, 22, 10, 0, 11, 16, 18, 1, 7, 1, 0, 7, 0, 0, 0, 0, 0, 7, 1, 1, 0, 0, 0, 1, 0, 31, 0, 0, 0, 0, 1, 1], [0, 0, 0, 3, 3, 0, 0, 5, 8, 2, 3, 4, 1, 1, 0, 0, 0, 0, 8, 7, 1, 0, 0, 5, 0, 2, 19, 8, 0, 9, 8, 13, 0, 4, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 28, 0, 0, 0, 0, 2, 1], [0, 0, 0, 1, 0, 0, 0, 3, 9, 0, 2, 1, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 2, 7, 1, 0, 1, 1, 0, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 1, 0], [0, 0, 0, 1, 0, 0, 0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 5, 3, 0, 2, 1, 3, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 0, 1, 0, 1, 1], [0, 0, 0, 1, 1, 2, 0, 5, 6, 1, 3, 1, 0, 0, 1, 0, 0, 1, 9, 1, 1, 2, 0, 0, 2, 0, 9, 2, 0, 7, 7, 8, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 3, 0], [1, 0, 0, 2, 2, 0, 1, 8, 6, 0, 4, 4, 1, 0, 0, 0, 1, 0, 9, 3, 0, 4, 0, 0, 0, 0, 12, 7, 0, 13, 5, 7, 0, 8, 1, 0, 2, 0, 0, 1, 0, 2, 9, 0, 0, 0, 1, 1, 0, 1, 20, 2, 0, 0, 3, 1, 3], [0, 0, 0, 2, 2, 0, 0, 5, 2, 1, 3, 1, 0, 1, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 10, 5, 0, 3, 5, 3, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 11, 0, 0, 0, 0, 2, 1], [0, 0, 0, 1, 4, 0, 1, 17, 11, 0, 7, 4, 0, 0, 0, 0, 0, 0, 16, 14, 0, 0, 0, 0, 0, 1, 16, 8, 0, 11, 11, 14, 0, 8, 1, 0, 2, 0, 0, 0, 0, 0, 4, 0, 1, 0, 4, 0, 0, 0, 30, 0, 0, 0, 1, 2, 7], [0, 0, 0, 2, 2, 0, 0, 7, 7, 0, 0, 2, 0, 0, 0, 1, 0, 0, 4, 5, 0, 0, 0, 0, 0, 1, 9, 3, 0, 4, 5, 6, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 2, 0, 0, 0, 13, 0, 0, 0, 0, 2, 1], [0, 1, 0, 2, 4, 1, 0, 2, 9, 2, 3, 4, 0, 0, 1, 0, 0, 0, 6, 3, 0, 1, 0, 0, 0, 1, 14, 5, 1, 6, 5, 9, 0, 7, 1, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 21, 0, 1, 2, 1, 1, 2], [0, 0, 0, 3, 3, 0, 0, 10, 8, 1, 5, 2, 1, 1, 0, 0, 0, 0, 11, 4, 0, 0, 0, 0, 0, 0, 19, 8, 0, 5, 5, 8, 0, 5, 1, 0, 1, 0, 0, 2, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 19, 0, 0, 0, 0, 5, 2]]\n",
      "\n",
      "\n",
      "TF-ISF vector: [0.16394124666183488, 0.38617775940636895, 0.46721507870087997, 0.25822637166754814, 0.21548407241684547, 0.3316636272224507, 0.6101138648932768, 0.5307336201098878, 0.18101258047078475, 0.20145644796380094, 0.35395047037203897, 0.48416708138675046, 0.19875189149086206, 0.5266469772058007, 0.2768554714744554, 0.39981084851327053, 0.35682082623259087]\n",
      "\n",
      "\n",
      "Max TF-ISF: [0.6101138648932768]\n",
      "\n",
      "\n",
      "Centroid: [1, 0, 0, 4, 1, 1, 0, 6, 8, 2, 4, 0, 0, 0, 0, 2, 0, 0, 4, 1, 0, 0, 0, 0, 0, 1, 7, 4, 0, 6, 2, 4, 2, 6, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 2, 2]\n",
      "\n",
      "\n",
      "Cosine Similarity Vector: [0.8983323694127738, 0.9295703982601832, 0.8992632993093933, 0.824958314243094, 0.9226939114637503, 1.0, 0.8608440338896084, 0.8886314755514175, 0.8957981849489617, 0.9069413063044849, 0.8655626644768962, 0.8990040968868919, 0.869236402705349, 0.8971124941168901, 0.9074044269686075, 0.9152379534467388, 0.9033706846949987]\n",
      "\n",
      "\n",
      "Sentence length feature vector: [0.25, 0.4583333333333333, 0.75, 0.4166666666666667, 0.2916666666666667, 0.2916666666666667, 1.0, 0.5416666666666666, 0.25, 0.20833333333333334, 0.375, 0.6666666666666666, 0.3333333333333333, 0.7083333333333334, 0.3333333333333333, 0.5833333333333334, 0.5416666666666666]\n",
      "\n",
      "\n",
      "Numeric token feature vector: [0.0, 0.0, 0.05555555555555555, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.07142857142857142, 0.0]\n",
      "\n",
      "\n",
      "Thematic word feature [0.02631578947368421, 0.05263157894736842, 0.08771929824561403, 0.07017543859649122, 0.03508771929824561, 0.043859649122807015, 0.11403508771929824, 0.09649122807017543, 0.03508771929824561, 0.017543859649122806, 0.03508771929824561, 0.06140350877192982, 0.043859649122807015, 0.10526315789473684, 0.05263157894736842, 0.06140350877192982, 0.06140350877192982]\n",
      "\n",
      "\n",
      "Pronoun feature vector [0.16666666666666666, 0.0, 0.1111111111111111, 0.0, 0.14285714285714285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5555555555555556, 0.5625, 0.0, 0.0, 0.0, 0.42857142857142855, 0.0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Printing Feature Matrix : \n",
      "[[1.00000000e+00 5.00000000e+00 4.00000000e+00 1.63941247e-01\n",
      "  8.98332369e-01 2.63157895e-02 2.50000000e-01 0.00000000e+00\n",
      "  1.66666667e-01]\n",
      " [9.41176471e-01 1.00000000e+01 9.00000000e+00 3.86177759e-01\n",
      "  9.29570398e-01 5.26315789e-02 4.58333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [8.82352941e-01 1.70000000e+01 1.60000000e+01 4.67215079e-01\n",
      "  8.99263299e-01 8.77192982e-02 7.50000000e-01 5.55555556e-02\n",
      "  1.11111111e-01]\n",
      " [8.23529412e-01 9.00000000e+00 8.00000000e+00 2.58226372e-01\n",
      "  8.24958314e-01 7.01754386e-02 4.16666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [7.64705882e-01 6.00000000e+00 5.00000000e+00 2.15484072e-01\n",
      "  9.22693911e-01 3.50877193e-02 2.91666667e-01 0.00000000e+00\n",
      "  1.42857143e-01]\n",
      " [7.05882353e-01 6.00000000e+00 5.00000000e+00 3.31663627e-01\n",
      "  1.00000000e+00 4.38596491e-02 2.91666667e-01 1.42857143e-01\n",
      "  0.00000000e+00]\n",
      " [6.47058824e-01 2.30000000e+01 2.20000000e+01 6.10113865e-01\n",
      "  8.60844034e-01 1.14035088e-01 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.88235294e-01 1.20000000e+01 1.10000000e+01 5.30733620e-01\n",
      "  8.88631476e-01 9.64912281e-02 5.41666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.29411765e-01 5.00000000e+00 4.00000000e+00 1.81012580e-01\n",
      "  8.95798185e-01 3.50877193e-02 2.50000000e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.70588235e-01 4.00000000e+00 3.00000000e+00 2.01456448e-01\n",
      "  9.06941306e-01 1.75438596e-02 2.08333333e-01 2.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [4.11764706e-01 8.00000000e+00 7.00000000e+00 3.53950470e-01\n",
      "  8.65562664e-01 3.50877193e-02 3.75000000e-01 0.00000000e+00\n",
      "  5.55555556e-01]\n",
      " [3.52941176e-01 1.50000000e+01 1.40000000e+01 4.84167081e-01\n",
      "  8.99004097e-01 6.14035088e-02 6.66666667e-01 6.25000000e-02\n",
      "  5.62500000e-01]\n",
      " [2.94117647e-01 7.00000000e+00 6.00000000e+00 1.98751891e-01\n",
      "  8.69236403e-01 4.38596491e-02 3.33333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.35294118e-01 1.60000000e+01 1.50000000e+01 5.26646977e-01\n",
      "  8.97112494e-01 1.05263158e-01 7.08333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.76470588e-01 7.00000000e+00 6.00000000e+00 2.76855471e-01\n",
      "  9.07404427e-01 5.26315789e-02 3.33333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.17647059e-01 1.30000000e+01 1.20000000e+01 3.99810849e-01\n",
      "  9.15237953e-01 6.14035088e-02 5.83333333e-01 7.14285714e-02\n",
      "  4.28571429e-01]\n",
      " [1.00000000e+00 1.20000000e+01 1.10000000e+01 3.56820826e-01\n",
      "  9.03370685e-01 6.14035088e-02 5.41666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "\n",
      "\n",
      "\n",
      "Printing Feature Matrix Normed : \n",
      "[[1.00000000e+00 5.00000000e+00 4.00000000e+00 1.63941247e-01\n",
      "  8.98332369e-01 2.63157895e-02 2.50000000e-01 0.00000000e+00\n",
      "  1.66666667e-01]\n",
      " [9.41176471e-01 1.00000000e+01 9.00000000e+00 3.86177759e-01\n",
      "  9.29570398e-01 5.26315789e-02 4.58333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [8.82352941e-01 1.70000000e+01 1.60000000e+01 4.67215079e-01\n",
      "  8.99263299e-01 8.77192982e-02 7.50000000e-01 5.55555556e-02\n",
      "  1.11111111e-01]\n",
      " [8.23529412e-01 9.00000000e+00 8.00000000e+00 2.58226372e-01\n",
      "  8.24958314e-01 7.01754386e-02 4.16666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [7.64705882e-01 6.00000000e+00 5.00000000e+00 2.15484072e-01\n",
      "  9.22693911e-01 3.50877193e-02 2.91666667e-01 0.00000000e+00\n",
      "  1.42857143e-01]\n",
      " [7.05882353e-01 6.00000000e+00 5.00000000e+00 3.31663627e-01\n",
      "  1.00000000e+00 4.38596491e-02 2.91666667e-01 1.42857143e-01\n",
      "  0.00000000e+00]\n",
      " [6.47058824e-01 2.30000000e+01 2.20000000e+01 6.10113865e-01\n",
      "  8.60844034e-01 1.14035088e-01 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.88235294e-01 1.20000000e+01 1.10000000e+01 5.30733620e-01\n",
      "  8.88631476e-01 9.64912281e-02 5.41666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.29411765e-01 5.00000000e+00 4.00000000e+00 1.81012580e-01\n",
      "  8.95798185e-01 3.50877193e-02 2.50000000e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.70588235e-01 4.00000000e+00 3.00000000e+00 2.01456448e-01\n",
      "  9.06941306e-01 1.75438596e-02 2.08333333e-01 2.00000000e-01\n",
      "  0.00000000e+00]\n",
      " [4.11764706e-01 8.00000000e+00 7.00000000e+00 3.53950470e-01\n",
      "  8.65562664e-01 3.50877193e-02 3.75000000e-01 0.00000000e+00\n",
      "  5.55555556e-01]\n",
      " [3.52941176e-01 1.50000000e+01 1.40000000e+01 4.84167081e-01\n",
      "  8.99004097e-01 6.14035088e-02 6.66666667e-01 6.25000000e-02\n",
      "  5.62500000e-01]\n",
      " [2.94117647e-01 7.00000000e+00 6.00000000e+00 1.98751891e-01\n",
      "  8.69236403e-01 4.38596491e-02 3.33333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.35294118e-01 1.60000000e+01 1.50000000e+01 5.26646977e-01\n",
      "  8.97112494e-01 1.05263158e-01 7.08333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.76470588e-01 7.00000000e+00 6.00000000e+00 2.76855471e-01\n",
      "  9.07404427e-01 5.26315789e-02 3.33333333e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.17647059e-01 1.30000000e+01 1.20000000e+01 3.99810849e-01\n",
      "  9.15237953e-01 6.14035088e-02 5.83333333e-01 7.14285714e-02\n",
      "  4.28571429e-01]\n",
      " [1.00000000e+00 1.20000000e+01 1.10000000e+01 3.56820826e-01\n",
      "  9.03370685e-01 6.14035088e-02 5.41666667e-01 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "[1.         5.         4.         0.16394125 0.89833237 0.02631579\n",
      " 0.25       0.         0.16666667]\n",
      "[ 0.94117647 10.          9.          0.38617776  0.9295704   0.05263158\n",
      "  0.45833333  0.          0.        ]\n",
      "[ 0.88235294 17.         16.          0.46721508  0.8992633   0.0877193\n",
      "  0.75        0.05555556  0.11111111]\n",
      "[0.82352941 9.         8.         0.25822637 0.82495831 0.07017544\n",
      " 0.41666667 0.         0.        ]\n",
      "[0.76470588 6.         5.         0.21548407 0.92269391 0.03508772\n",
      " 0.29166667 0.         0.14285714]\n",
      "[0.70588235 6.         5.         0.33166363 1.         0.04385965\n",
      " 0.29166667 0.14285714 0.        ]\n",
      "[ 0.64705882 23.         22.          0.61011386  0.86084403  0.11403509\n",
      "  1.          0.          0.        ]\n",
      "[ 0.58823529 12.         11.          0.53073362  0.88863148  0.09649123\n",
      "  0.54166667  0.          0.        ]\n",
      "[0.52941176 5.         4.         0.18101258 0.89579818 0.03508772\n",
      " 0.25       0.         0.        ]\n",
      "[0.47058824 4.         3.         0.20145645 0.90694131 0.01754386\n",
      " 0.20833333 0.2        0.        ]\n",
      "[0.41176471 8.         7.         0.35395047 0.86556266 0.03508772\n",
      " 0.375      0.         0.55555556]\n",
      "[ 0.35294118 15.         14.          0.48416708  0.8990041   0.06140351\n",
      "  0.66666667  0.0625      0.5625    ]\n",
      "[0.29411765 7.         6.         0.19875189 0.8692364  0.04385965\n",
      " 0.33333333 0.         0.        ]\n",
      "[ 0.23529412 16.         15.          0.52664698  0.89711249  0.10526316\n",
      "  0.70833333  0.          0.        ]\n",
      "[0.17647059 7.         6.         0.27685547 0.90740443 0.05263158\n",
      " 0.33333333 0.         0.        ]\n",
      "[ 0.11764706 13.         12.          0.39981085  0.91523795  0.06140351\n",
      "  0.58333333  0.07142857  0.42857143]\n",
      "[ 1.         12.         11.          0.35682083  0.90337068  0.06140351\n",
      "  0.54166667  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# # Import text file from the user \n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "root = Tk()\n",
    "my_filetypes = [('all files', '.*'), ('text files', '.txt')]\n",
    "root.filename =  filedialog.askopenfilename(\n",
    "                                    title=\"Please select a file:\",\n",
    "                                    filetypes=my_filetypes)\n",
    "print (\"File directory:\",root.filename)\n",
    "print(\"\\n\")\n",
    "root.withdraw()\n",
    "text=open(root.filename).read()\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence segmentation\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences=(sent_tokenize(text))\n",
    "print(\"Sentences:\",sentences)\n",
    "print(\"\\n\")\n",
    "#print(len(sentences))\n",
    "\n",
    "\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "\n",
    "\n",
    "# # Tokenization, Stop word removal , Bi-grams, Tri-grams\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    \n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "print(\"bitokens feature vector:\",(bi_token_length))\n",
    "#print(max(bi_token_length))\n",
    "#print(bi_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "\n",
    "print(\"tritokens feature vector:\",tri_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "num_sent=len(sent_position)\n",
    "print(\"sentence position:\",sent_position)\n",
    "print(\"\\n\")\n",
    "print(\"Total number of sentences:\",num_sent)\n",
    "print(\"\\n\")\n",
    "#th= 0.2*num_sent\n",
    "#minv=th*num_sent\n",
    "#maxv=th*2*num_sent\n",
    "position = []\n",
    "position_rbm = []\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "for x in range(1,num_sent-1):\n",
    "\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "sent_pos2 = 100\n",
    "sent_pos2_rbm = 1\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "print(\"Sentence position feature vector:\",position_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "print(\"SentenceVectors:\",VSM)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "\n",
    "print(\"TF-ISF vector:\",tfisfvec_rbm)\n",
    "print(\"\\n\")\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "print(\"Max TF-ISF:\",centroid)\n",
    "print(\"\\n\")\n",
    "#for q in range(sentencelength):\n",
    "centroid=(max(VSM))\n",
    "print(\"Centroid:\",centroid)\n",
    "print(\"\\n\")\n",
    "\n",
    "# # Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "\n",
    "\n",
    "##OR BY THIS METHOD: LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "#print(sent_length)\n",
    "\n",
    "print(\"Sentence length feature vector:\",sent_length_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "#print(\"Numeric word count in each sentence:\",num_word)\n",
    "#print(\"\\n\")\n",
    "print(\"Numeric token feature vector:\",numeric_token_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Thematic words feature\n",
    "from rake_nltk import Rake\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "keywords=[]\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "# r = Rake(<list of stopwords>, <string of puntuations to ignore>)\n",
    "\n",
    "for s in sentences:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    key=list(r.get_ranked_phrases())\n",
    "    keywords.append(key)\n",
    "#print(keywords)\n",
    "l_keywords=[]\n",
    "for s in keywords:\n",
    "    leng=len(s)\n",
    "    l_keywords.append(leng)\n",
    "#print(l_keywords)\n",
    "\n",
    "total_keywords=sum(l_keywords)\n",
    "#print(total_keywords)\n",
    "\n",
    "thematic_number= []\n",
    "thematic_number_rbm= []\n",
    "for x in l_keywords:\n",
    "    thematic_number.append((x/total_keywords)*100)\n",
    "    thematic_number_rbm.append(x/total_keywords)\n",
    "print(\"Thematic word feature\", thematic_number_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # proper noun feature\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "pncounts = []\n",
    "pncounts_rbm = []\n",
    "for sentence in sentences:\n",
    "    tagged=nltk.pos_tag(nltk.word_tokenize(str(sentence)))\n",
    "    counts = Counter(tag for word,tag in tagged if tag.startswith('NNP') or tag.startswith('NNPS'))\n",
    "    f=sum(counts.values())\n",
    "    pncounts.append(f)\n",
    "    pncounts_rbm.append(f)\n",
    "pnounscore=[(int(o) / int(p))*100 for o,p in zip(pncounts, sent_word)]\n",
    "pnounscore_rbm=[int(o) / int(p) for o,p in zip(pncounts_rbm, sent_word)]\n",
    "#print(pncounts)\n",
    "print(\"Pronoun feature vector\",pnounscore_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# # feature matrix1\n",
    "\n",
    "\n",
    "featureMatrix = []\n",
    "featureMatrix.append(position_rbm)\n",
    "featureMatrix.append(bi_token_length)\n",
    "featureMatrix.append(tri_token_length)\n",
    "featureMatrix.append(tfisfvec_rbm)\n",
    "featureMatrix.append(cosine_similarity_rbm)\n",
    "featureMatrix.append(thematic_number_rbm)\n",
    "featureMatrix.append(sent_length_rbm)\n",
    "featureMatrix.append(numeric_token_rbm)\n",
    "featureMatrix.append(pnounscore_rbm)\n",
    "\n",
    "\n",
    "\n",
    "featureMat = np.zeros((len(sentences),9))\n",
    "for i in range(9) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]\n",
    "\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix : \")\n",
    "print(featureMat)\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix Normed : \")\n",
    "#featureMat_normed = featureMat / featureMat.max(axis=0)\n",
    "featureMat_normed = featureMat\n",
    "\n",
    "print(featureMat_normed)\n",
    "for i in range(len(sentences)):\n",
    "    print(featureMat_normed[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy logic summary \n",
      "\n",
      " ['The Food Safety and Standards Authority of India (FSSAI) is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry.', 'A notification to create a separate category of food business operators (FBOs), who will be licensed to deal only with leftover food, has been drafted to ensure the quality of food.', '\"We are looking at partnering with NGOs or organisations that collect, store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food,\" said Pawan Agarwal, CEO of FSSAI.', 'We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties,\"  says Pawan Agarwal, \"All food, whether it is paid for or distributed free, must meet the country\\'s food safety and hygiene standards,\" he said.', 'Mechanical engineering.', '\"We will have a central helpline number.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# functions\n",
    "position1       = ctrl.Antecedent(np.arange(0, 100, 10), 'position1')\n",
    "cos_similarity  = ctrl.Antecedent(np.arange(0, 100, 10), 'cos_similarity')\n",
    "bitokens        = ctrl.Antecedent(np.arange(0, 100, 10), 'bitokens')\n",
    "tritokens       = ctrl.Antecedent(np.arange(0, 100, 10), 'tritokens')\n",
    "propernoun      = ctrl.Antecedent(np.arange(0, 100, 10), 'propernoun')\n",
    "sentencelength  = ctrl.Antecedent(np.arange(0, 100, 10), 'sentencelength')\n",
    "numtokens       = ctrl.Antecedent(np.arange(0, 100, 10), 'numtokens')\n",
    "keywords        = ctrl.Antecedent(np.arange(0, 10, 1), 'keywords')\n",
    "tf_isf          = ctrl.Antecedent(np.arange(0, 100, 10), 'tf_isf')\n",
    "\n",
    "\n",
    "senten = ctrl.Consequent(np.arange(0, 100, 10), 'senten')\n",
    "\n",
    "position1.automf(3)\n",
    "cos_similarity.automf(3)\n",
    "bitokens.automf(3)\n",
    "tritokens.automf(3)\n",
    "propernoun.automf(3)\n",
    "sentencelength.automf(3)\n",
    "numtokens.automf(3)\n",
    "keywords.automf(3)\n",
    "tf_isf.automf(3)\n",
    "\n",
    "\n",
    "senten['bad'] = fuzz.trimf(senten.universe, [0, 0, 50])\n",
    "senten['avg'] = fuzz.trimf(senten.universe, [0, 50, 100])\n",
    "senten['good'] = fuzz.trimf(senten.universe, [50, 100, 100])\n",
    "\n",
    "rule1 = ctrl.Rule(position1['good'] & sentencelength['good'] & propernoun['good'] &numtokens['good'], senten['good'])\n",
    "rule2 = ctrl.Rule(position1['poor'] & sentencelength['poor'] & numtokens['poor'], senten['bad'])\n",
    "rule3 = ctrl.Rule(propernoun['poor'] & keywords['average'], senten['bad'])\n",
    "rule4 = ctrl.Rule(cos_similarity['good'], senten['good'])\n",
    "rule5 = ctrl.Rule(bitokens['good'] & tritokens['good'] & numtokens['average'] | tf_isf['average'], senten['avg'])\n",
    "\n",
    "\n",
    "sent_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5])\n",
    "Sent = ctrl.ControlSystemSimulation(sent_ctrl)\n",
    "fuzzemptyarr= np.empty((20,1,2), dtype=object)\n",
    "t2=0\n",
    "summary2=[]\n",
    "for s in range(len(sentences)):\n",
    "    Sent.input['position1'] = int(position[s])\n",
    "    Sent.input['cos_similarity'] = int(cosine_similarity[s])\n",
    "    Sent.input['bitokens'] = int(bi_tokens[s])\n",
    "    Sent.input['tritokens'] = int(tri_tokens[s])\n",
    "    Sent.input['tf_isf'] = int(tfisfvec[s])\n",
    "    Sent.input['keywords'] = int(thematic_number[s])\n",
    "    Sent.input['propernoun'] = int(pnounscore[s])\n",
    "    Sent.input['sentencelength'] = int(sent_length[s])\n",
    "    Sent.input['numtokens'] = int(numeric_token[s])\n",
    "#Sent.input['service'] = 2\n",
    "    Sent.compute()\n",
    "    if Sent.output['senten'] > 50:\n",
    "        summary2.append((sentences[s]))\n",
    "        fuzzemptyarr[t2][0][0] = sentences[s]\n",
    "        fuzzemptyarr[t2][0][1] = s\n",
    "        t2+=1\n",
    "fuzzarray = np.empty((len(summary2),1,2),dtype=object)\n",
    "for i in range(len(summary2)):\n",
    "    fuzzarray[i][0][0] = fuzzemptyarr[i][0][0]\n",
    "    fuzzarray[i][0][1] = fuzzemptyarr[i][0][1]\n",
    "    \n",
    "fuzzarray=fuzzarray[1:]\n",
    "print(\"Fuzzy logic summary \\n\\n\",summary2)\n",
    "#print(len(summary2))\n",
    "#print(fuzzarray)\n",
    "    #senten.view(sim=Sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
