{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File directory: C:/Users/HP/Desktop/SIH/fuzzy/doc3.txt\n",
      "\n",
      "\n",
      "The Food Safety and Standards Authority of India (FSSAI) is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry. A notification to create a separate category of food business operators (FBOs), who will be licensed to deal only with leftover food, has been drafted to ensure the quality of food. \"We are looking at partnering with NGOs or organisations that collect, store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food,\" said Pawan Agarwal, CEO of FSSAI. \"Tonnes of food is wasted annually. We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties,\"  says Pawan Agarwal, \"All food, whether it is paid for or distributed free, must meet the country's food safety and hygiene standards,\" he said. Mechanical engineering. The organisations in the business of collecting leftover food will now have to work in collaboration with FSSAI so their efforts can be scaled up.?Tonnes of food is wasted annually and can be used to feed several thousands. We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties etc., said Agarwal. The initiative will set up a helpline network where organisations can call in for collection but reaching individuals who want to directly donate food will take time. \"We will have a central helpline number. #%^&%&$$vhkvkhvfhxdhxxfbvbmv%#. Reaching people at the household level may not be feasible initially but it is an integral part of the long-term plan,\" he said. \"We have begun collecting names of people working in the sector. There are still a few months to go before the scheme materialises,\" said Agarwal. \"Collecting food going waste to feed the hungry is a noble thought but to transport, store and maintain the cold chain of cooked food is a huge challenge. Text summarization. The logistics are a nightmare, which is why we don't handle leftovers and only distribute uncooked food that can be cooked locally,\" said Kuldip Nar, founder of Delhi NCR Food Bank, which has been feeding the poor in 10 cities since 2011.\n",
      "\n",
      "\n",
      "Sentences: ['The Food Safety and Standards Authority of India (FSSAI) is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry.', 'A notification to create a separate category of food business operators (FBOs), who will be licensed to deal only with leftover food, has been drafted to ensure the quality of food.', '\"We are looking at partnering with NGOs or organisations that collect, store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food,\" said Pawan Agarwal, CEO of FSSAI.', '\"Tonnes of food is wasted annually.', 'We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties,\"  says Pawan Agarwal, \"All food, whether it is paid for or distributed free, must meet the country\\'s food safety and hygiene standards,\" he said.', 'Mechanical engineering.', 'The organisations in the business of collecting leftover food will now have to work in collaboration with FSSAI so their efforts can be scaled up.', '?Tonnes of food is wasted annually and can be used to feed several thousands.', 'We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties etc., said Agarwal.', 'The initiative will set up a helpline network where organisations can call in for collection but reaching individuals who want to directly donate food will take time.', '\"We will have a central helpline number.', '#%^&%&$$vhkvkhvfhxdhxxfbvbmv%#.', 'Reaching people at the household level may not be feasible initially but it is an integral part of the long-term plan,\" he said.', '\"We have begun collecting names of people working in the sector.', 'There are still a few months to go before the scheme materialises,\" said Agarwal.', '\"Collecting food going waste to feed the hungry is a noble thought but to transport, store and maintain the cold chain of cooked food is a huge challenge.', 'Text summarization.', 'The logistics are a nightmare, which is why we don\\'t handle leftovers and only distribute uncooked food that can be cooked locally,\" said Kuldip Nar, founder of Delhi NCR Food Bank, which has been feeding the poor in 10 cities since 2011.']\n",
      "\n",
      "\n",
      "bitokens feature vector: [19, 15, 22, 3, 25, 1, 10, 7, 11, 14, 3, 0, 11, 6, 6, 16, 1, 21]\n",
      "\n",
      "\n",
      "tritokens feature vector: [18, 14, 21, 2, 24, 0, 9, 6, 10, 13, 2, 0, 10, 5, 5, 15, 0, 20]\n",
      "\n",
      "\n",
      "sentence position: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "\n",
      "\n",
      "Total number of sentences: 18\n",
      "\n",
      "\n",
      "Sentence position feature vector: [1, 0.9444444444444444, 0.8888888888888888, 0.8333333333333334, 0.7777777777777778, 0.7222222222222222, 0.6666666666666666, 0.6111111111111112, 0.5555555555555556, 0.5, 0.4444444444444444, 0.3888888888888889, 0.3333333333333333, 0.2777777777777778, 0.2222222222222222, 0.16666666666666666, 0.1111111111111111, 1]\n",
      "\n",
      "\n",
      "SentenceVectors: [[0, 4, 0, 10, 1, 0, 0, 17, 3, 16, 2, 0, 5, 0, 0, 0, 0, 2, 8, 2, 13, 2, 4, 1, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 13, 2, 15, 1, 1, 0, 3, 0, 0, 12, 5, 1, 0, 13, 4, 0, 34, 0, 0, 3, 0], [0, 4, 0, 8, 0, 0, 0, 19, 3, 14, 3, 1, 4, 0, 0, 0, 0, 1, 8, 1, 8, 0, 0, 1, 0, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 7, 0, 19, 0, 1, 0, 3, 0, 0, 7, 1, 1, 0, 11, 7, 2, 30, 1, 0, 2, 1], [2, 3, 0, 13, 1, 0, 0, 13, 4, 16, 4, 0, 8, 0, 1, 0, 0, 2, 3, 1, 12, 1, 2, 0, 0, 11, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 17, 1, 14, 0, 0, 0, 2, 1, 0, 9, 6, 1, 1, 20, 7, 3, 33, 0, 0, 2, 2], [1, 0, 0, 1, 0, 0, 0, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 1, 0, 0, 1, 0, 0, 2, 0, 1, 0, 3, 2, 0, 5, 0, 0, 0, 0], [3, 8, 1, 13, 5, 0, 0, 14, 5, 17, 5, 0, 10, 0, 0, 0, 0, 2, 7, 0, 16, 0, 0, 0, 0, 16, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 11, 1, 24, 0, 0, 0, 4, 1, 0, 13, 7, 1, 0, 22, 8, 7, 42, 0, 0, 2, 0], [0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0], [0, 5, 0, 10, 0, 0, 0, 15, 2, 9, 4, 0, 5, 0, 0, 0, 0, 1, 5, 1, 6, 1, 2, 0, 0, 8, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 9, 1, 11, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 7, 8, 0, 24, 0, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 6, 3, 3, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 7, 0, 8, 1, 0, 0, 1, 0, 0, 6, 0, 1, 0, 7, 3, 0, 13, 0, 0, 0, 0], [0, 8, 1, 7, 3, 0, 0, 7, 2, 8, 3, 0, 5, 0, 0, 0, 0, 1, 2, 0, 9, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 13, 0, 0, 0, 0, 1, 0, 5, 6, 2, 0, 14, 6, 3, 20, 0, 0, 1, 0], [0, 6, 0, 17, 1, 0, 0, 11, 3, 13, 6, 0, 5, 0, 0, 0, 0, 0, 2, 0, 6, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 12, 2, 14, 1, 0, 0, 1, 0, 0, 5, 2, 1, 0, 11, 12, 0, 26, 0, 0, 2, 0], [1, 1, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 0, 6, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 5, 0, 6, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0], [1, 1, 1, 9, 2, 0, 0, 6, 2, 10, 0, 0, 6, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 7, 0, 14, 0, 0, 0, 2, 0, 0, 2, 3, 1, 0, 10, 10, 1, 22, 0, 0, 4, 0], [1, 3, 0, 3, 1, 0, 0, 5, 1, 3, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 5, 1, 9, 0, 0, 0, 0, 1, 0, 0, 3, 1, 0, 2, 3, 0, 10, 0, 0, 2, 0], [1, 1, 0, 4, 3, 0, 0, 4, 0, 5, 2, 0, 4, 0, 0, 0, 0, 1, 2, 0, 5, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 11, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 7, 4, 1, 13, 0, 0, 0, 0], [1, 5, 0, 7, 1, 0, 0, 16, 4, 13, 1, 0, 8, 0, 1, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 1, 12, 0, 0, 0, 1, 0, 0, 6, 7, 1, 0, 9, 6, 1, 27, 0, 0, 1, 0], [0, 0, 0, 2, 2, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0], [1, 9, 0, 16, 1, 0, 0, 17, 4, 10, 4, 1, 12, 1, 1, 2, 0, 0, 5, 1, 7, 0, 0, 0, 0, 9, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 3, 3, 13, 3, 20, 1, 0, 1, 3, 0, 0, 12, 3, 1, 2, 12, 9, 4, 41, 0, 0, 2, 0]]\n",
      "\n",
      "\n",
      "TF-ISF vector: [0.5275099452518809, 0.4849073176279059, 0.537006686577765, 0.13779246992914812, 0.6400265448059564, 0.19614703787497903, 0.38373082376350354, 0.268100118835413, 0.37264549804142555, 0.4247835236658766, 0.14209590718878642, 0.9228981336124191, 0.34405248785904896, 0.17958346800258565, 0.23193066524268566, 0.391721052326256, 0.17557776101893746, 0.6750144592249856]\n",
      "\n",
      "\n",
      "Max TF-ISF: [0.9228981336124191]\n",
      "\n",
      "\n",
      "Centroid: [3, 8, 1, 13, 5, 0, 0, 14, 5, 17, 5, 0, 10, 0, 0, 0, 0, 2, 7, 0, 16, 0, 0, 0, 0, 16, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 11, 1, 24, 0, 0, 0, 4, 1, 0, 13, 7, 1, 0, 22, 8, 7, 42, 0, 0, 2, 0]\n",
      "\n",
      "\n",
      "Cosine Similarity Vector: [0.961227946436427, 0.9560509370404991, 0.9720757650450443, 0.837932757912274, 1.0, 0.6218594191630815, 0.931760483681483, 0.9155797354413161, 0.9793734779200461, 0.9256001531303252, 0.815685475392602, 0.12259471484224266, 0.9361352396082051, 0.8953046298493887, 0.9562184164815065, 0.9438806117309608, 0.6769064842612652, 0.9581222391844202]\n",
      "\n",
      "\n",
      "Sentence length feature vector: [0.7692307692307693, 0.6153846153846154, 0.8846153846153846, 0.15384615384615385, 1.0, 0.07692307692307693, 0.4230769230769231, 0.3076923076923077, 0.46153846153846156, 0.5769230769230769, 0.15384615384615385, 0.038461538461538464, 0.46153846153846156, 0.2692307692307692, 0.2692307692307692, 0.6538461538461539, 0.07692307692307693, 0.8461538461538461]\n",
      "\n",
      "\n",
      "Numeric token feature vector: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09090909090909091]\n",
      "\n",
      "\n",
      "Thematic word feature [0.10077519379844961, 0.09302325581395349, 0.10077519379844961, 0.023255813953488372, 0.13953488372093023, 0.007751937984496124, 0.06201550387596899, 0.03875968992248062, 0.06976744186046512, 0.07751937984496124, 0.007751937984496124, 0.007751937984496124, 0.05426356589147287, 0.023255813953488372, 0.031007751937984496, 0.07751937984496124, 0.007751937984496124, 0.07751937984496124]\n",
      "\n",
      "\n",
      "Pronoun feature vector [0.3, 0.0625, 0.21739130434782608, 0.0, 0.07692307692307693, 0.0, 0.09090909090909091, 0.0, 0.08333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.14285714285714285, 0.0, 0.0, 0.2727272727272727]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Printing Feature Matrix : \n",
      "[[1.00000000e+00 1.90000000e+01 1.80000000e+01 5.27509945e-01\n",
      "  9.61227946e-01 1.00775194e-01 7.69230769e-01 0.00000000e+00\n",
      "  3.00000000e-01]\n",
      " [9.44444444e-01 1.50000000e+01 1.40000000e+01 4.84907318e-01\n",
      "  9.56050937e-01 9.30232558e-02 6.15384615e-01 0.00000000e+00\n",
      "  6.25000000e-02]\n",
      " [8.88888889e-01 2.20000000e+01 2.10000000e+01 5.37006687e-01\n",
      "  9.72075765e-01 1.00775194e-01 8.84615385e-01 0.00000000e+00\n",
      "  2.17391304e-01]\n",
      " [8.33333333e-01 3.00000000e+00 2.00000000e+00 1.37792470e-01\n",
      "  8.37932758e-01 2.32558140e-02 1.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [7.77777778e-01 2.50000000e+01 2.40000000e+01 6.40026545e-01\n",
      "  1.00000000e+00 1.39534884e-01 1.00000000e+00 0.00000000e+00\n",
      "  7.69230769e-02]\n",
      " [7.22222222e-01 1.00000000e+00 0.00000000e+00 1.96147038e-01\n",
      "  6.21859419e-01 7.75193798e-03 7.69230769e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [6.66666667e-01 1.00000000e+01 9.00000000e+00 3.83730824e-01\n",
      "  9.31760484e-01 6.20155039e-02 4.23076923e-01 0.00000000e+00\n",
      "  9.09090909e-02]\n",
      " [6.11111111e-01 7.00000000e+00 6.00000000e+00 2.68100119e-01\n",
      "  9.15579735e-01 3.87596899e-02 3.07692308e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.55555556e-01 1.10000000e+01 1.00000000e+01 3.72645498e-01\n",
      "  9.79373478e-01 6.97674419e-02 4.61538462e-01 0.00000000e+00\n",
      "  8.33333333e-02]\n",
      " [5.00000000e-01 1.40000000e+01 1.30000000e+01 4.24783524e-01\n",
      "  9.25600153e-01 7.75193798e-02 5.76923077e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.44444444e-01 3.00000000e+00 2.00000000e+00 1.42095907e-01\n",
      "  8.15685475e-01 7.75193798e-03 1.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [3.88888889e-01 0.00000000e+00 0.00000000e+00 9.22898134e-01\n",
      "  1.22594715e-01 7.75193798e-03 3.84615385e-02 0.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [3.33333333e-01 1.10000000e+01 1.00000000e+01 3.44052488e-01\n",
      "  9.36135240e-01 5.42635659e-02 4.61538462e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.77777778e-01 6.00000000e+00 5.00000000e+00 1.79583468e-01\n",
      "  8.95304630e-01 2.32558140e-02 2.69230769e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.22222222e-01 6.00000000e+00 5.00000000e+00 2.31930665e-01\n",
      "  9.56218416e-01 3.10077519e-02 2.69230769e-01 0.00000000e+00\n",
      "  1.42857143e-01]\n",
      " [1.66666667e-01 1.60000000e+01 1.50000000e+01 3.91721052e-01\n",
      "  9.43880612e-01 7.75193798e-02 6.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.11111111e-01 1.00000000e+00 0.00000000e+00 1.75577761e-01\n",
      "  6.76906484e-01 7.75193798e-03 7.69230769e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.00000000e+00 2.10000000e+01 2.00000000e+01 6.75014459e-01\n",
      "  9.58122239e-01 7.75193798e-02 8.46153846e-01 9.09090909e-02\n",
      "  2.72727273e-01]]\n",
      "\n",
      "\n",
      "\n",
      "Printing Feature Matrix Normed : \n",
      "[[1.00000000e+00 1.90000000e+01 1.80000000e+01 5.27509945e-01\n",
      "  9.61227946e-01 1.00775194e-01 7.69230769e-01 0.00000000e+00\n",
      "  3.00000000e-01]\n",
      " [9.44444444e-01 1.50000000e+01 1.40000000e+01 4.84907318e-01\n",
      "  9.56050937e-01 9.30232558e-02 6.15384615e-01 0.00000000e+00\n",
      "  6.25000000e-02]\n",
      " [8.88888889e-01 2.20000000e+01 2.10000000e+01 5.37006687e-01\n",
      "  9.72075765e-01 1.00775194e-01 8.84615385e-01 0.00000000e+00\n",
      "  2.17391304e-01]\n",
      " [8.33333333e-01 3.00000000e+00 2.00000000e+00 1.37792470e-01\n",
      "  8.37932758e-01 2.32558140e-02 1.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [7.77777778e-01 2.50000000e+01 2.40000000e+01 6.40026545e-01\n",
      "  1.00000000e+00 1.39534884e-01 1.00000000e+00 0.00000000e+00\n",
      "  7.69230769e-02]\n",
      " [7.22222222e-01 1.00000000e+00 0.00000000e+00 1.96147038e-01\n",
      "  6.21859419e-01 7.75193798e-03 7.69230769e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [6.66666667e-01 1.00000000e+01 9.00000000e+00 3.83730824e-01\n",
      "  9.31760484e-01 6.20155039e-02 4.23076923e-01 0.00000000e+00\n",
      "  9.09090909e-02]\n",
      " [6.11111111e-01 7.00000000e+00 6.00000000e+00 2.68100119e-01\n",
      "  9.15579735e-01 3.87596899e-02 3.07692308e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.55555556e-01 1.10000000e+01 1.00000000e+01 3.72645498e-01\n",
      "  9.79373478e-01 6.97674419e-02 4.61538462e-01 0.00000000e+00\n",
      "  8.33333333e-02]\n",
      " [5.00000000e-01 1.40000000e+01 1.30000000e+01 4.24783524e-01\n",
      "  9.25600153e-01 7.75193798e-02 5.76923077e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [4.44444444e-01 3.00000000e+00 2.00000000e+00 1.42095907e-01\n",
      "  8.15685475e-01 7.75193798e-03 1.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [3.88888889e-01 0.00000000e+00 0.00000000e+00 9.22898134e-01\n",
      "  1.22594715e-01 7.75193798e-03 3.84615385e-02 0.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [3.33333333e-01 1.10000000e+01 1.00000000e+01 3.44052488e-01\n",
      "  9.36135240e-01 5.42635659e-02 4.61538462e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.77777778e-01 6.00000000e+00 5.00000000e+00 1.79583468e-01\n",
      "  8.95304630e-01 2.32558140e-02 2.69230769e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.22222222e-01 6.00000000e+00 5.00000000e+00 2.31930665e-01\n",
      "  9.56218416e-01 3.10077519e-02 2.69230769e-01 0.00000000e+00\n",
      "  1.42857143e-01]\n",
      " [1.66666667e-01 1.60000000e+01 1.50000000e+01 3.91721052e-01\n",
      "  9.43880612e-01 7.75193798e-02 6.53846154e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.11111111e-01 1.00000000e+00 0.00000000e+00 1.75577761e-01\n",
      "  6.76906484e-01 7.75193798e-03 7.69230769e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.00000000e+00 2.10000000e+01 2.00000000e+01 6.75014459e-01\n",
      "  9.58122239e-01 7.75193798e-02 8.46153846e-01 9.09090909e-02\n",
      "  2.72727273e-01]]\n",
      "[ 1.         19.         18.          0.52750995  0.96122795  0.10077519\n",
      "  0.76923077  0.          0.3       ]\n",
      "[ 0.94444444 15.         14.          0.48490732  0.95605094  0.09302326\n",
      "  0.61538462  0.          0.0625    ]\n",
      "[ 0.88888889 22.         21.          0.53700669  0.97207577  0.10077519\n",
      "  0.88461538  0.          0.2173913 ]\n",
      "[0.83333333 3.         2.         0.13779247 0.83793276 0.02325581\n",
      " 0.15384615 0.         0.        ]\n",
      "[ 0.77777778 25.         24.          0.64002654  1.          0.13953488\n",
      "  1.          0.          0.07692308]\n",
      "[0.72222222 1.         0.         0.19614704 0.62185942 0.00775194\n",
      " 0.07692308 0.         0.        ]\n",
      "[ 0.66666667 10.          9.          0.38373082  0.93176048  0.0620155\n",
      "  0.42307692  0.          0.09090909]\n",
      "[0.61111111 7.         6.         0.26810012 0.91557974 0.03875969\n",
      " 0.30769231 0.         0.        ]\n",
      "[ 0.55555556 11.         10.          0.3726455   0.97937348  0.06976744\n",
      "  0.46153846  0.          0.08333333]\n",
      "[ 0.5        14.         13.          0.42478352  0.92560015  0.07751938\n",
      "  0.57692308  0.          0.        ]\n",
      "[0.44444444 3.         2.         0.14209591 0.81568548 0.00775194\n",
      " 0.15384615 0.         0.        ]\n",
      "[0.38888889 0.         0.         0.92289813 0.12259471 0.00775194\n",
      " 0.03846154 0.         1.        ]\n",
      "[ 0.33333333 11.         10.          0.34405249  0.93613524  0.05426357\n",
      "  0.46153846  0.          0.        ]\n",
      "[0.27777778 6.         5.         0.17958347 0.89530463 0.02325581\n",
      " 0.26923077 0.         0.        ]\n",
      "[0.22222222 6.         5.         0.23193067 0.95621842 0.03100775\n",
      " 0.26923077 0.         0.14285714]\n",
      "[ 0.16666667 16.         15.          0.39172105  0.94388061  0.07751938\n",
      "  0.65384615  0.          0.        ]\n",
      "[0.11111111 1.         0.         0.17557776 0.67690648 0.00775194\n",
      " 0.07692308 0.         0.        ]\n",
      "[ 1.         21.         20.          0.67501446  0.95812224  0.07751938\n",
      "  0.84615385  0.09090909  0.27272727]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# # Import text file from the user \n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "root = Tk()\n",
    "my_filetypes = [('all files', '.*'), ('text files', '.txt')]\n",
    "root.filename =  filedialog.askopenfilename(\n",
    "                                    title=\"Please select a file:\",\n",
    "                                    filetypes=my_filetypes)\n",
    "print (\"File directory:\",root.filename)\n",
    "print(\"\\n\")\n",
    "root.withdraw()\n",
    "text=open(root.filename).read()\n",
    "print(text)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence segmentation\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences=(sent_tokenize(text))\n",
    "print(\"Sentences:\",sentences)\n",
    "print(\"\\n\")\n",
    "#print(len(sentences))\n",
    "\n",
    "\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "\n",
    "\n",
    "# # Tokenization, Stop word removal , Bi-grams, Tri-grams\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    \n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "print(\"bitokens feature vector:\",(bi_token_length))\n",
    "#print(max(bi_token_length))\n",
    "#print(bi_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "\n",
    "print(\"tritokens feature vector:\",tri_token_length)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "num_sent=len(sent_position)\n",
    "print(\"sentence position:\",sent_position)\n",
    "print(\"\\n\")\n",
    "print(\"Total number of sentences:\",num_sent)\n",
    "print(\"\\n\")\n",
    "#th= 0.2*num_sent\n",
    "#minv=th*num_sent\n",
    "#maxv=th*2*num_sent\n",
    "position = []\n",
    "position_rbm = []\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "for x in range(1,num_sent-1):\n",
    "\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "sent_pos2 = 100\n",
    "sent_pos2_rbm = 1\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "print(\"Sentence position feature vector:\",position_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "print(\"SentenceVectors:\",VSM)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "\n",
    "print(\"TF-ISF vector:\",tfisfvec_rbm)\n",
    "print(\"\\n\")\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "print(\"Max TF-ISF:\",centroid)\n",
    "print(\"\\n\")\n",
    "#for q in range(sentencelength):\n",
    "centroid=(max(VSM))\n",
    "print(\"Centroid:\",centroid)\n",
    "print(\"\\n\")\n",
    "\n",
    "# # Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "\n",
    "\n",
    "##OR BY THIS METHOD: LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "#print(sent_length)\n",
    "\n",
    "print(\"Sentence length feature vector:\",sent_length_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "#print(\"Numeric word count in each sentence:\",num_word)\n",
    "#print(\"\\n\")\n",
    "print(\"Numeric token feature vector:\",numeric_token_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Thematic words feature\n",
    "from rake_nltk import Rake\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "keywords=[]\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "# r = Rake(<list of stopwords>, <string of puntuations to ignore>)\n",
    "\n",
    "for s in sentences:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    key=list(r.get_ranked_phrases())\n",
    "    keywords.append(key)\n",
    "#print(keywords)\n",
    "l_keywords=[]\n",
    "for s in keywords:\n",
    "    leng=len(s)\n",
    "    l_keywords.append(leng)\n",
    "#print(l_keywords)\n",
    "\n",
    "total_keywords=sum(l_keywords)\n",
    "#print(total_keywords)\n",
    "\n",
    "thematic_number= []\n",
    "thematic_number_rbm= []\n",
    "for x in l_keywords:\n",
    "    thematic_number.append((x/total_keywords)*100)\n",
    "    thematic_number_rbm.append(x/total_keywords)\n",
    "print(\"Thematic word feature\", thematic_number_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # proper noun feature\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "pncounts = []\n",
    "pncounts_rbm = []\n",
    "for sentence in sentences:\n",
    "    tagged=nltk.pos_tag(nltk.word_tokenize(str(sentence)))\n",
    "    counts = Counter(tag for word,tag in tagged if tag.startswith('NNP') or tag.startswith('NNPS'))\n",
    "    f=sum(counts.values())\n",
    "    pncounts.append(f)\n",
    "    pncounts_rbm.append(f)\n",
    "pnounscore=[(int(o) / int(p))*100 for o,p in zip(pncounts, sent_word)]\n",
    "pnounscore_rbm=[int(o) / int(p) for o,p in zip(pncounts_rbm, sent_word)]\n",
    "#print(pncounts)\n",
    "print(\"Pronoun feature vector\",pnounscore_rbm)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# # feature matrix1\n",
    "\n",
    "\n",
    "featureMatrix = []\n",
    "featureMatrix.append(position_rbm)\n",
    "featureMatrix.append(bi_token_length)\n",
    "featureMatrix.append(tri_token_length)\n",
    "featureMatrix.append(tfisfvec_rbm)\n",
    "featureMatrix.append(cosine_similarity_rbm)\n",
    "featureMatrix.append(thematic_number_rbm)\n",
    "featureMatrix.append(sent_length_rbm)\n",
    "featureMatrix.append(numeric_token_rbm)\n",
    "featureMatrix.append(pnounscore_rbm)\n",
    "\n",
    "\n",
    "\n",
    "featureMat = np.zeros((len(sentences),9))\n",
    "for i in range(9) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]\n",
    "\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix : \")\n",
    "print(featureMat)\n",
    "print(\"\\n\\n\\nPrinting Feature Matrix Normed : \")\n",
    "#featureMat_normed = featureMat / featureMat.max(axis=0)\n",
    "featureMat_normed = featureMat\n",
    "\n",
    "print(featureMat_normed)\n",
    "for i in range(len(sentences)):\n",
    "    print(featureMat_normed[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy logic summary \n",
      "\n",
      " ['The Food Safety and Standards Authority of India (FSSAI) is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry.', 'A notification to create a separate category of food business operators (FBOs), who will be licensed to deal only with leftover food, has been drafted to ensure the quality of food.', '\"We are looking at partnering with NGOs or organisations that collect, store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food,\" said Pawan Agarwal, CEO of FSSAI.', 'We are looking at creating a mechanism through which food can be collected from restaurants, weddings, large-scale parties,\"  says Pawan Agarwal, \"All food, whether it is paid for or distributed free, must meet the country\\'s food safety and hygiene standards,\" he said.', '\"We will have a central helpline number.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# functions\n",
    "position1       = ctrl.Antecedent(np.arange(0, 100, 10), 'position1')\n",
    "cos_similarity  = ctrl.Antecedent(np.arange(0, 100, 10), 'cos_similarity')\n",
    "bitokens        = ctrl.Antecedent(np.arange(0, 100, 10), 'bitokens')\n",
    "tritokens       = ctrl.Antecedent(np.arange(0, 100, 10), 'tritokens')\n",
    "propernoun      = ctrl.Antecedent(np.arange(0, 100, 10), 'propernoun')\n",
    "sentencelength  = ctrl.Antecedent(np.arange(0, 100, 10), 'sentencelength')\n",
    "numtokens       = ctrl.Antecedent(np.arange(0, 100, 10), 'numtokens')\n",
    "keywords        = ctrl.Antecedent(np.arange(0, 10, 1), 'keywords')\n",
    "tf_isf          = ctrl.Antecedent(np.arange(0, 100, 10), 'tf_isf')\n",
    "\n",
    "\n",
    "senten = ctrl.Consequent(np.arange(0, 100, 10), 'senten')\n",
    "\n",
    "position1.automf(3)\n",
    "cos_similarity.automf(3)\n",
    "bitokens.automf(3)\n",
    "tritokens.automf(3)\n",
    "propernoun.automf(3)\n",
    "sentencelength.automf(3)\n",
    "numtokens.automf(3)\n",
    "keywords.automf(3)\n",
    "tf_isf.automf(3)\n",
    "\n",
    "\n",
    "senten['bad'] = fuzz.trimf(senten.universe, [0, 0, 50])\n",
    "senten['avg'] = fuzz.trimf(senten.universe, [0, 50, 100])\n",
    "senten['good'] = fuzz.trimf(senten.universe, [50, 100, 100])\n",
    "\n",
    "rule1 = ctrl.Rule(position1['good'] & sentencelength['good'] & propernoun['good'] &numtokens['good'], senten['good'])\n",
    "rule2 = ctrl.Rule(position1['poor'] & sentencelength['poor'] & numtokens['poor'], senten['bad'])\n",
    "rule3 = ctrl.Rule(propernoun['poor'] & keywords['average'], senten['bad'])\n",
    "rule4 = ctrl.Rule(cos_similarity['good'], senten['good'])\n",
    "rule5 = ctrl.Rule(bitokens['good'] & tritokens['good'] & numtokens['average'] | tf_isf['average'], senten['avg'])\n",
    "\n",
    "\n",
    "sent_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5])\n",
    "Sent = ctrl.ControlSystemSimulation(sent_ctrl)\n",
    "fuzzemptyarr= np.empty((20,1,2), dtype=object)\n",
    "t2=0\n",
    "summary2=[]\n",
    "for s in range(len(sentences)):\n",
    "    Sent.input['position1'] = int(position[s])\n",
    "    Sent.input['cos_similarity'] = int(cosine_similarity[s])\n",
    "    Sent.input['bitokens'] = int(bi_tokens[s])\n",
    "    Sent.input['tritokens'] = int(tri_tokens[s])\n",
    "    Sent.input['tf_isf'] = int(tfisfvec[s])\n",
    "    Sent.input['keywords'] = int(thematic_number[s])\n",
    "    Sent.input['propernoun'] = int(pnounscore[s])\n",
    "    Sent.input['sentencelength'] = int(sent_length[s])\n",
    "    Sent.input['numtokens'] = int(numeric_token[s])\n",
    "#Sent.input['service'] = 2\n",
    "    Sent.compute()\n",
    "    if Sent.output['senten'] > 50:\n",
    "        summary2.append((sentences[s]))\n",
    "        fuzzemptyarr[t2][0][0] = sentences[s]\n",
    "        fuzzemptyarr[t2][0][1] = s\n",
    "        t2+=1\n",
    "fuzzarray = np.empty((len(summary2),1,2),dtype=object)\n",
    "for i in range(len(summary2)):\n",
    "    fuzzarray[i][0][0] = fuzzemptyarr[i][0][0]\n",
    "    fuzzarray[i][0][1] = fuzzemptyarr[i][0][1]\n",
    "    \n",
    "fuzzarray=fuzzarray[1:]\n",
    "print(\"Fuzzy logic summary \\n\\n\",summary2)\n",
    "#print(len(summary2))\n",
    "#print(fuzzarray)\n",
    "    #senten.view(sim=Sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
